{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/songhan/pruning/\n",
      "lenet5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.vq as scv\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# os.system(\"cd $CAFFE_ROOT\")\n",
    "caffe_root = os.environ[\"CAFFE_ROOT\"]\n",
    "os.chdir(caffe_root)\n",
    "print caffe_root\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(2)\n",
    "option = 'lenet5'\n",
    "if option == 'lenet5':\n",
    "#     prototxt = '3_prototxt_solver/lenet5/train_val.prototxt'             \n",
    "    caffemodel = '4_model_checkpoint/lenet5/lenet5.caffemodel'\n",
    "    solver_proto = '3_prototxt_solver/lenet5/lenet_solver_finetune.prototxt'\n",
    "    iters = 100\n",
    "    dir_t = '2_results/kmeans/lenet5/'\n",
    "elif option == 'alexnet':\n",
    "    prototxt = '3_prototxt_solver/L2/train_val.prototxt'             \n",
    "    caffemodel = '4_model_checkpoint/alexnet/alexnet9x.caffemodel'  \n",
    "    solver_proto = '3_prototxt_solver/L2/finetune_solver.prototxt'\n",
    "    iters = 1000\n",
    "    dir_t = '2_results/kmeans/alexnet/'\n",
    "elif option == 'vgg':\n",
    "    prototxt = '3_prototxt_solver/vgg16/train_val.prototxt'             \n",
    "    caffemodel = '4_model_checkpoint/vgg16/vgg16_12x.caffemodel'  \n",
    "    iters = 1000\n",
    "    dir_t = '2_results/kmeans/vgg16/'\n",
    "\n",
    "log = dir_t + 'log_accu'\n",
    "print option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers TBD:  ['conv1', 'conv2', 'ip1', 'ip2']\n",
      "num_c =  [8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "solver = caffe.SGDSolver(solver_proto)\n",
    "solver.net.copy_from(caffemodel)\n",
    "net = solver.net\n",
    "\n",
    "layers = [\"conv1\", \"conv2\", \"ip1\", \"ip2\"]\n",
    "num_c = [8, 8, 8, 8]\n",
    "\n",
    "# layers = [\"ip2\"]\n",
    "# num_c =[4]\n",
    "print \"layers TBD: \", layers\n",
    "print \"num_c = \", num_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============1 Perform K-means=============\n",
      "Eval layer: conv1\n",
      "codebook: [ 0.         -0.29499978 -0.03177995  0.25980002  0.51812828]\n",
      "codebook size: 5\n",
      "Eval layer: conv2\n",
      "codebook: [ 0.         -0.14003672 -0.08655009 -0.04095863  0.05826917  0.12395576\n",
      "  0.21972357]\n",
      "codebook size: 7\n",
      "Eval layer: ip1\n",
      "codebook: [ 0.         -0.07194122 -0.04054011 -0.01950156  0.01798702  0.03304647\n",
      "  0.05246379  0.08222321]\n",
      "codebook size: 8\n",
      "Eval layer: ip2\n",
      "codebook: [ 0.         -0.25988275 -0.17693673 -0.10709237  0.13464746  0.23517904]\n",
      "codebook size: 6\n"
     ]
    }
   ],
   "source": [
    "print \"==============1 Perform K-means=============\"\n",
    "codebook = {}\n",
    "for idx, layer in enumerate(layers):\n",
    "    print \"Eval layer:\", layer\n",
    "    W = net.params[layer][0].data.flatten()\n",
    "    W = W[np.where(W != 0)]\n",
    "    std = np.std(W)\n",
    "    initial_uni = np.linspace(-4 * std, 4 * std, num_c[idx]-1)\n",
    "    codebook[layer],_= scv.kmeans(W, initial_uni)    \n",
    "    codebook[layer] = np.append(0.0, codebook[layer])\n",
    "    print \"codebook:\", codebook[layer]\n",
    "    print \"codebook size:\", len(codebook[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3 3] [ 2.  8.  6.  6.] [ 0 10 20 30 40]\n"
     ]
    }
   ],
   "source": [
    "def stochasitc_quantize(W, codebook):\n",
    "#     print \"codebook\", codebook\n",
    "#     print \"W is:   \", W\n",
    "    codebook=np.append(codebook,[np.inf])\n",
    "    code0, dist0 = scv.vq(W, codebook)\n",
    "\n",
    "    code1=[]\n",
    "    dist1=[]\n",
    "    for i in xrange(len(W)):\n",
    "        d1=abs(W[i]-codebook[code0[i]-1])\n",
    "        d2=abs(W[i]-codebook[code0[i]+1])\n",
    "        if (d1>d2):\n",
    "            code1.append(code0[i]+1)\n",
    "            dist1.append(d2)\n",
    "        else:\n",
    "            code1.append(code0[i]-1)\n",
    "            dist1.append(d1)\n",
    "\n",
    "    r = np.random.uniform(low=0.0, high=1.0, size=(len(W)))\n",
    "\n",
    "    sum_dist=dist0+dist1\n",
    "    x=dist0/sum_dist\n",
    "\n",
    "    mask0 = r>x\n",
    "    mask1 = -mask0\n",
    "\n",
    "    final_code = mask0*code0 + mask1*code1\n",
    "    final_dist = mask0*dist0 + mask1*dist1\n",
    "#     print \"code0 is:\", code0\n",
    "#     print \"dist0 is:\", dist0\n",
    "#     print \"code1 is:\", code1\n",
    "#     print \"dist1 is:\", dist1\n",
    "#     print dist0, r\n",
    "#     print mask0\n",
    "#     print mask1\n",
    "    return final_code, final_dist\n",
    "    \n",
    "\n",
    "# for test:\n",
    "codebook_ = np.array([0, 10, 20, 30, 40])\n",
    "W_= np.array([12, 18, 24, 36])\n",
    "final_code, final_dist = stochasitc_quantize( W_, codebook_)\n",
    "print final_code, final_dist, codebook_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================2 Perform quantization==============\n"
     ]
    }
   ],
   "source": [
    "def quantize(net, layers, use_stochastic=False):\n",
    "    print \"================2 Perform quantization==============\"\n",
    "    codeDict={}\n",
    "    maskCode={}\n",
    "    for layer in layers:\n",
    "#         print \"Quantize layer:\", layer\n",
    "        W = net.params[layer][0].data\n",
    "        if use_stochastic:\n",
    "            codes, dist = stochasitc_quantize(W.flatten(), codebook[layer])        \n",
    "        else:\n",
    "            codes, dist = scv.vq(W.flatten(), codebook[layer])\n",
    "\n",
    "        W_q = np.reshape(codebook[layer][codes], W.shape)\n",
    "        net.params[layer][0].data[...] = W_q\n",
    "\n",
    "        maskCode[layer] = np.reshape(codes, W.shape)\n",
    "        codeBookSize = len(codebook[layer])    \n",
    "#         print \"W_q.shape=\", W_q.shape        \n",
    "#         print \"codebook length=\", codeBookSize\n",
    "#         print \"maskcode:\", maskCode[layer].flatten().shape\n",
    "        a = maskCode[layer].flatten()\n",
    "        b = xrange(len(a))\n",
    "        codeDict[layer]={}\n",
    "        for i in xrange(len(a)):\n",
    "            codeDict[layer].setdefault(a[i], []).append(b[i])\n",
    "\n",
    "    return codeDict, maskCode\n",
    "\n",
    "codeDict, maskCode = quantize(net, layers, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================3 Perform fintuning==============\n",
      "iteration: 499 requantize...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-8f36ed0ad8a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mcontinue\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodeDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mdiff_ave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mupdate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sgd'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print \"================3 Perform fintuning==============\"\n",
    "# print codebook\n",
    "learning_rate=1e-3\n",
    "decay_rate = 0.99 \n",
    "momentum=0.9\n",
    "update='rmsprop'\n",
    "use_stochastic=False\n",
    "\n",
    "\n",
    "import time\n",
    "start_time=time.time()\n",
    "step_cache={}\n",
    "for i in xrange(10000):\n",
    "    solver.step(1)\n",
    "    for layer in layers:\n",
    "        if not layer in step_cache: \n",
    "            step_cache[layer]={}\n",
    "        diff=net.params[layer][0].diff.flatten()\n",
    "#         W1 =  net.params[layer][0].data\n",
    "        codeBookSize=len(codebook[layer])\n",
    "        for code in xrange(codeBookSize):\n",
    "            if code==0: continue;\n",
    "            indexes = codeDict[layer][code]\n",
    "            diff_ave=np.sum(diff[indexes])/len(indexes)\n",
    "\n",
    "            if update == 'sgd':\n",
    "                dx = -learning_rate * diff_ave\n",
    "            elif update == 'momentum':\n",
    "                if not code in step_cache[layer]:\n",
    "                    step_cache[layer][code] = 0\n",
    "                dx = momentum * step_cache[layer][code] - learning_rate * diff_ave\n",
    "                step_cache[layer][code] = dx                \n",
    "            elif update == 'rmsprop':\n",
    "                if not code in step_cache[layer]:\n",
    "                    step_cache[layer][code] = 0\n",
    "                step_cache[layer][code] =  decay_rate * step_cache[layer][code] + (1.0 - decay_rate) * diff_ave ** 2\n",
    "                dx = -(learning_rate * diff_ave) / np.sqrt(step_cache[layer][code] + 1e-8)\n",
    "            elif update == 'adagrad':\n",
    "                if not code in step_cache[layer]:\n",
    "                    step_cache[layer][code] = 0\n",
    "                step_cache[layer][code] +=  diff_ave ** 2\n",
    "                dx = -(learning_rate * diff_ave) / np.sqrt(step_cache[layer][code] + 1e-8)\n",
    "            \n",
    "            codebook[layer][code] += dx\n",
    "        W2 = codebook[layer][maskCode[layer]]\n",
    "\n",
    "#         if lr==0:\n",
    "#             assert ((W1==W2).all())\n",
    "        \n",
    "        net.params[layer][0].data[...]=W2\n",
    "\n",
    "    if (i+1)%500==0:\n",
    "        print \"iteration:\", i, \"requantize...\"\n",
    "        if use_stochastic:\n",
    "            codeDict, maskCode = quantize(net, layers, True)\n",
    "    if (i+1)%3000==0:\n",
    "        learning_rate=learning_rate/10\n",
    "        print \"learning rate = \", learning_rate\n",
    "    \n",
    "\n",
    "print \"time elapsed: \", time.time()-start_time \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print net.params['ip2'][0].data[:10,0:3]\n",
    "print codebook['ip2'][maskCode['ip2']][:10,0:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original net:\n",
    "Test net output #0: accuracy = 0.992\n",
    "\n",
    "after pruning to 8% parameters left and quantization all parameters to 3bits:\n",
    "Test net output #0: accuracy = 0.9891\n",
    "\n",
    "after retraining the codeBook:\n",
    "     update == 'rmsprop':\n",
    "     25 seconds\n",
    "     lr = 1e-3 or 1e-2\n",
    "Test net output #0: accuracy = 0.9914\n",
    "accuracy loss = 0.06%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print \"============ fine tune without codebook on Training Set =========\"\n",
    "print \"batch size=\", net.blobs['label'].data.shape\n",
    "print \"batch size=\", solver.test_nets[0].blobs['label'].data.shape\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for i in xrange(5000):\n",
    "    solver.step(1)\n",
    "        \n",
    "print time.time()-start_time"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# print \"============ Test Accuracy on Training Set =========\"\n",
    "# correct = 0\n",
    "# for test_it in range(50000/64):\n",
    "#     net.forward()\n",
    "#     correct += sum(net.blobs['ip2'].data.argmax(1)\n",
    "#                    == net.blobs['label'].data)\n",
    "# print correct / float(50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
