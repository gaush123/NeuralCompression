{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/songhan/pruning/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.vq as scv\n",
    "import pickle\n",
    "\n",
    "# os.system(\"cd $CAFFE_ROOT\")\n",
    "caffe_root = os.environ[\"CAFFE_ROOT\"]\n",
    "os.chdir(caffe_root)\n",
    "print caffe_root\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(2)\n",
    "option = 'lenet5'\n",
    "if option == 'lenet5':\n",
    "    prototxt = '3_prototxt_solver/lenet5/train_val.prototxt'             \n",
    "    caffemodel = '4_model_checkpoint/lenet5/lenet5.caffemodel'\n",
    "    iters = 100\n",
    "    dir_t = '2_results/kmeans/lenet5/'\n",
    "elif option == 'alexnet':\n",
    "    prototxt = '3_prototxt_solver/L2/train_val.prototxt'             \n",
    "    caffemodel = '4_model_checkpoint/alexnet/alexnet9x.caffemodel'  \n",
    "    iters = 1000\n",
    "    dir_t = '2_results/kmeans/alexnet/'\n",
    "elif option == 'vgg':\n",
    "    prototxt = '3_prototxt_solver/vgg16/train_val.prototxt'             \n",
    "    caffemodel = '4_model_checkpoint/vgg16/vgg16_12x.caffemodel'  \n",
    "    iters = 1000\n",
    "    dir_t = '2_results/kmeans/vgg16/'\n",
    "\n",
    "log = dir_t + 'log_accu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers TBD:  ['conv1', 'conv2', 'ip1', 'ip2']\n",
      "num_c =  [8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "choice = [64,16]\n",
    "net = caffe.Net(prototxt, caffemodel, caffe.TRAIN)\n",
    "\n",
    "layers = [\"conv1\", \"conv2\", \"ip1\", \"ip2\"]\n",
    "num_c = [8, 8, 8, 8]\n",
    "\n",
    "# layers = [\"ip2\"]\n",
    "# num_c =[4]\n",
    "print \"layers TBD: \", layers\n",
    "print \"num_c = \", num_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============1 Perform K-means=============\n",
      "Eval layer: conv1\n",
      "codebook: [ 0.         -0.29499978 -0.03177995  0.25980002  0.51812828]\n",
      "codebook size: 5\n",
      "Eval layer: conv2\n",
      "codebook: [ 0.         -0.14003672 -0.08655009 -0.04095863  0.05826917  0.12395576\n",
      "  0.21972357]\n",
      "codebook size: 7\n",
      "Eval layer: ip1\n",
      "codebook: [ 0.         -0.07194122 -0.04054011 -0.01950156  0.01798702  0.03304647\n",
      "  0.05246379  0.08222321]\n",
      "codebook size: 8\n",
      "Eval layer: ip2\n",
      "codebook: [ 0.         -0.25988275 -0.17693673 -0.10709237  0.13464746  0.23517904]\n",
      "codebook size: 6\n"
     ]
    }
   ],
   "source": [
    "print \"==============1 Perform K-means=============\"\n",
    "codebook = {}\n",
    "for idx, layer in enumerate(layers):\n",
    "    print \"Eval layer:\", layer\n",
    "    W = net.params[layer][0].data.flatten()\n",
    "    W = W[np.where(W != 0)]\n",
    "    std = np.std(W)\n",
    "    initial_uni = np.linspace(-4 * std, 4 * std, num_c[idx]-1)\n",
    "    codebook[layer],_= scv.kmeans(W, initial_uni)    \n",
    "    codebook[layer] = np.append(0.0, codebook[layer])\n",
    "    print \"codebook:\", codebook[layer]\n",
    "    print \"codebook size:\", len(codebook[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================2 Perform quantization==============\n",
      "Quantize layer: conv1\n",
      "W_q.shape= (20, 1, 5, 5)\n",
      "codebook length= 5\n",
      "maskcode: (500,)\n",
      "maskcode: (280,)\n",
      "Quantize layer: conv2\n",
      "W_q.shape= (50, 20, 5, 5)\n",
      "codebook length= 7\n",
      "maskcode: (25000,)\n",
      "maskcode: (2728,)\n",
      "Quantize layer: ip1\n",
      "W_q.shape= (500, 800)\n",
      "codebook length= 8\n",
      "maskcode: (400000,)\n",
      "maskcode: (30875,)\n",
      "Quantize layer: ip2\n",
      "W_q.shape= (10, 500)\n",
      "codebook length= 6\n",
      "maskcode: (5000,)\n",
      "maskcode: (958,)\n"
     ]
    }
   ],
   "source": [
    "print \"================2 Perform quantization==============\"\n",
    "codeDict={}\n",
    "maskCode={}\n",
    "for layer in layers:\n",
    "    print \"Quantize layer:\", layer\n",
    "    W = net.params[layer][0].data\n",
    "    codes, dist = scv.vq(W.flatten(), codebook[layer])\n",
    "    W_q = np.reshape(codebook[layer][codes], W.shape)\n",
    "    net.params[layer][0].data[...] = W_q\n",
    "\n",
    "    maskCode[layer] = np.reshape(codes, W.shape)\n",
    "    codeBookSize = len(codebook[layer])    \n",
    "    print \"W_q.shape=\", W_q.shape        \n",
    "    print \"codebook length=\", codeBookSize\n",
    "    print \"maskcode:\", maskCode[layer].flatten().shape\n",
    "    print \"maskcode:\", np.flatnonzero(maskCode[layer]).shape\n",
    "    a = maskCode[layer].flatten()\n",
    "    b = xrange(len(a))\n",
    "#     print a\n",
    "#     print b\n",
    "\n",
    "    codeDict[layer]={}\n",
    "    for i in xrange(len(a)):\n",
    "        codeDict[layer].setdefault(a[i], []).append(b[i])\n",
    "#     print \"codeDict  is\",codeDict\n",
    "#     print maskCode[layer]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================3 Perform fintuning==============\n",
      "iteration: 0 codebook: [ 0.         -0.25990896 -0.17694944 -0.10711319  0.13463184  0.23521768]\n",
      "iteration: 200 codebook: [ 0.         -0.26377599 -0.1814741  -0.11143175  0.13921597  0.23923433]\n",
      "iteration: 400 codebook: [ 0.         -0.26640841 -0.18419175 -0.11412992  0.1419364   0.24178728]\n",
      "iteration: 600 codebook: [ 0.         -0.26865944 -0.18660347 -0.11640809  0.14427963  0.24407232]\n",
      "iteration: 800 codebook: [ 0.         -0.27076142 -0.18888835 -0.11860535  0.14656414  0.2459383 ]\n",
      "iteration: 1000 codebook: [ 0.         -0.27279905 -0.19110092 -0.12074002  0.14874192  0.2477962 ]\n",
      "iteration: 1200 codebook: [ 0.         -0.27485651 -0.19320269 -0.12287282  0.150913    0.24944088]\n",
      "iteration: 1400 codebook: [ 0.         -0.27690424 -0.19528797 -0.12503956  0.15302393  0.25120896]\n",
      "iteration: 1600 codebook: [ 0.         -0.27888461 -0.19737946 -0.12713386  0.15510681  0.25265595]\n",
      "iteration: 1800 codebook: [ 0.         -0.28079498 -0.19946814 -0.12919623  0.15717838  0.25116665]\n",
      "iteration: 2000 codebook: [ 0.         -0.28270972 -0.20155657 -0.13122823  0.15921153  0.24870061]\n",
      "iteration: 2200 codebook: [ 0.         -0.28465108 -0.20359302 -0.1332499   0.16127695  0.24642868]\n",
      "iteration: 2400 codebook: [ 0.         -0.28660939 -0.20563605 -0.13528503  0.16329452  0.24456143]\n",
      "iteration: 2600 codebook: [ 0.         -0.2883772  -0.20768044 -0.13731591  0.1653059   0.24249857]\n",
      "iteration: 2800 codebook: [ 0.         -0.29013689 -0.20973268 -0.13929937  0.16729715  0.24040718]\n",
      "time elapsed:  30.419727087\n",
      "============ Test Accuracy on Training Set =========\n",
      "0.99678\n"
     ]
    }
   ],
   "source": [
    "print \"================3 Perform fintuning==============\"\n",
    "# print codebook\n",
    "learning_rate=1e-5\n",
    "decay_rate = 0.99 \n",
    "momentum=0.9\n",
    "update='rmsprop'\n",
    "import time\n",
    "start_time=time.time()\n",
    "step_cache={}\n",
    "for i in xrange(3000):\n",
    "    net.forward()\n",
    "    net.backward()\n",
    "    for layer in layers:\n",
    "        if not layer in step_cache: \n",
    "            step_cache[layer]={}\n",
    "        diff=net.params[layer][0].diff.flatten()\n",
    "        W1 =  net.params[layer][0].data\n",
    "        codeBookSize=len(codebook[layer])\n",
    "        for code in xrange(codeBookSize):\n",
    "            if code==0: continue;\n",
    "            indexes = codeDict[layer][code]\n",
    "            diff_ave=np.sum(diff[indexes])/len(indexes)\n",
    "\n",
    "            if update == 'sgd':\n",
    "                dx = -learning_rate * diff_ave\n",
    "            elif update == 'momentum':\n",
    "                if not code in step_cache[layer]:\n",
    "                    step_cache[layer][code] = 0\n",
    "                dx = momentum * step_cache[layer][code] - learning_rate * diff_ave\n",
    "                step_cache[layer][code] = dx                \n",
    "            elif update == 'rmsprop':\n",
    "                if not code in step_cache[layer]:\n",
    "                    step_cache[layer][code] = 0\n",
    "                step_cache[layer][code] =  decay_rate * step_cache[layer][code] + (1.0 - decay_rate) * diff_ave ** 2\n",
    "                dx = -(learning_rate * diff_ave) / np.sqrt(step_cache[layer][code] + 1e-8)\n",
    "            elif update == 'adagrad':\n",
    "                if not code in step_cache[layer]:\n",
    "                    step_cache[layer][code] = 0\n",
    "                step_cache[layer][code] +=  diff_ave ** 2\n",
    "                dx = -(learning_rate * diff_ave) / np.sqrt(step_cache[layer][code] + 1e-8)\n",
    "            \n",
    "            codebook[layer][code] += dx\n",
    "        W2 = codebook[layer][maskCode[layer]]\n",
    "\n",
    "#         if lr==0:\n",
    "#             assert ((W1==W2).all())\n",
    "        \n",
    "        net.params[layer][0].data[...]=W2\n",
    "\n",
    "    if i%200==0:\n",
    "        print \"iteration:\", i, \"codebook:\", codebook[\"ip2\"]\n",
    "    \n",
    "\n",
    "print \"time elapsed: \", time.time()-start_time \n",
    "\n",
    "print \"============ Test Accuracy on Training Set =========\"\n",
    "correct = 0\n",
    "for test_it in range(50000/64):\n",
    "    net.forward()\n",
    "    correct += sum(net.blobs['ip2'].data.argmax(1)\n",
    "                   == net.blobs['label'].data)\n",
    "print correct / float(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original accuracy: 0.9984\n",
    "\n",
    "lr=1e-5, 3000 iterations:\n",
    "\n",
    "sgd: 0.99584 / time elapsed:  22.2082271576\n",
    "\n",
    "momentum: 0.99638 / time elapsed:  22.2547438145\n",
    "\n",
    "rmsprop: 0.99678 / time elapsed:  30.419727087\n",
    "\n",
    "adagrad: 0.9952 / time elapsed:  22.2563259602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"============ fine tune without codebook on Training Set =========\"\n",
    "print \"batch size=\",net.blobs['label'].data.shape\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for i in xrange(1000):\n",
    "    net.forward()\n",
    "    net.backward()\n",
    "    for layer in layers:        \n",
    "        diff=net.params[layer][0].diff\n",
    "        W=    net.params[layer][0].data\n",
    "        W -= 0.000001*diff    \n",
    "        net.params[layer][0].data[...]=W\n",
    "        \n",
    "print time.time()-start_time\n",
    "correct = 0\n",
    "for test_it in range(50000/64):\n",
    "    net.forward()\n",
    "    correct += sum(net.blobs['ip2'].data.argmax(1)\n",
    "                   == net.blobs['label'].data)\n",
    "print correct / float(50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
